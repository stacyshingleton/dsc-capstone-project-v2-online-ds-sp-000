{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook scrapes the top 1000 lifetime adjusted box office grosses from boxofficemojo.com. It exports the data as a list of dictionaries to 'mojo_cumulative_scraper_w_id.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_number_url_list = list(range(0, 1000, 200))\n",
    "page_number_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainpage_url_list = []\n",
    "\n",
    "mainpage_url = \\\n",
    "\"https://www.boxofficemojo.com/chart/top_lifetime_gross_adjusted/?adjust_gross_to=2020&offset=\"\n",
    "\n",
    "for i in page_number_url_list:\n",
    "    mainpage_url_list.append(mainpage_url + str(i))\n",
    "\n",
    "mainpage_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_each_movie_dict = []\n",
    "\n",
    "\n",
    "for i in mainpage_url_list:\n",
    "    mainpage = requests.get(i)\n",
    "    soup = BeautifulSoup(mainpage.content, 'html.parser') \n",
    "    grandparent = soup.findAll('tr')\n",
    "\n",
    "    for i in grandparent[1:]:\n",
    "        x = i.find('a', {'class':'a-link-normal'}).get('href')\n",
    "        id_slice_1 = str(x.split('/title/', 1)[1])\n",
    "        id_slice_2 = str(id_slice_1.split('/?ref_', 1)[0])\n",
    "        each_movie_mainpage_url = str('https://www.boxofficemojo.com' + x)\n",
    "        \n",
    "        movie_dict = {'adj_lifetime_gross': [],\n",
    "                      'lifetime_gross': [],\n",
    "                      'est_num_tickets': [],\n",
    "                      'movie_name': [],\n",
    "                      'production_budget':[],\n",
    "                      'mpaa_rating':[],\n",
    "                      'running_time': [],\n",
    "                      'movie_genre': [],\n",
    "                      'production_companies': [],\n",
    "                      'earliest_dom_release_date': [],\n",
    "                      'cast':[],\n",
    "                      'movie_id': []\n",
    "                     }\n",
    "\n",
    "        movie_dict['adj_lifetime_gross'].append(i.findAll \\\n",
    "                    ('td', {'class': 'a-text-right mojo-field-type-money'})[0].get_text())        \n",
    "        movie_dict['lifetime_gross'].append(i.findAll \\\n",
    "                    ('td', {'class': 'a-text-right mojo-field-type-money'})[1].get_text())\n",
    "        movie_dict['est_num_tickets'].append(i.find \\\n",
    "                    ('td', {'class': 'a-text-right mojo-field-type-positive_integer'}) \\\n",
    "                    .get_text())\n",
    "        \n",
    "        summarypage = requests.get(each_movie_mainpage_url)\n",
    "        soup2 = BeautifulSoup(summarypage.content, 'html.parser')\n",
    "\n",
    "\n",
    "        try:\n",
    "            production_company_string = (soup2.find('main').find('span', \\\n",
    "                    text = \"Domestic Distributor\").next_sibling.get_text()) \\\n",
    "                    .split('See full company information\\n\\n', 1)[0]\n",
    "            movie_dict['production_companies'].append(production_company_string)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            movie_dict['movie_name'].append(soup2.find('main').find \\\n",
    "                    ('h1', {'class': 'a-size-extra-large'}).get_text())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            movie_dict['production_budget'].append(soup2.find('main') \\\n",
    "                    .find('span', text = \"Budget\").next_sibling.get_text())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            movie_dict['mpaa_rating'].append(soup2.find('main') \\\n",
    "                    .find('span', text = \"MPAA\").next_sibling.get_text())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            movie_dict['running_time'].append(soup2.find('main') \n",
    "                    .find('span', text = \"Running Time\").next_sibling.get_text())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            earliest_dom_release_date_string = (soup2.find('main') \\\n",
    "                    .find('span', text = \"Earliest Release Date\") \\\n",
    "                    .next_sibling.get_text()).split('\\n', 1)[0]\n",
    "            movie_dict['earliest_dom_release_date'] \\\n",
    "                    .append(earliest_dom_release_date_string)\n",
    "        except:\n",
    "            pass   \n",
    "\n",
    "        try:\n",
    "            movie_genre_string = (soup2.find('main').find('span', \\\n",
    "                    text = \"Genres\").next_sibling.get_text()) \\\n",
    "                    .split('\\n    \\n        ')\n",
    "            movie_dict['movie_genre'].append(movie_genre_string)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        sep = '?'\n",
    "        text = each_movie_mainpage_url\n",
    "        rest = text.split(sep, 1)[0]\n",
    "        crew_url_each_movie = (rest + 'credits')\n",
    "        crewpage = requests.get(crew_url_each_movie)\n",
    "        soup3 = BeautifulSoup(crewpage.content, 'html.parser')\n",
    "        \n",
    "        td_crew_list = soup3.find('table').findAll('tr')\n",
    "        crew_member_dict = {}\n",
    "\n",
    "        for j in td_crew_list[1:]:\n",
    "            crew_name = j.get_text()\n",
    "            v,k = crew_name.split('\\n\\n')\n",
    "            if k in crew_member_dict:\n",
    "                crew_member_dict[k].append(v)\n",
    "            else:\n",
    "                crew_member_dict[k] = [v]\n",
    "        \n",
    "        movie_dict.update(crew_member_dict)\n",
    "        \n",
    "        \n",
    "        sep2 = '?'\n",
    "        text2 = each_movie_mainpage_url\n",
    "        rest2 = text.split(sep, 1)[0]\n",
    "        cast_url_each_movie = (rest2 + 'credits')\n",
    "        cast_list = []\n",
    "\n",
    "\n",
    "        castpage = requests.get(cast_url_each_movie)\n",
    "        soup4 = BeautifulSoup(castpage.content, 'html.parser')\n",
    "        td_cast_list = soup4.find('table', {'id':'principalCast'}).findAll('a')\n",
    "\n",
    "\n",
    "        for i in td_cast_list:\n",
    "            cast_string = i.get_text().replace('\\n\\n', '')\n",
    "            if cast_string != \"See more\":\n",
    "                cast_list.append(cast_string)                \n",
    "                \n",
    "        movie_dict['cast'].append(cast_list)\n",
    "        movie_dict['movie_id'].append(id_slice_2)\n",
    "    \n",
    "        print(movie_dict)\n",
    "\n",
    "        list_of_each_movie_dict.append(movie_dict)\n",
    "        \n",
    "with open('mojo_cumulative_scraper_w_id.txt', 'w') as outfile:\n",
    "    json.dump(list_of_each_movie_dict, outfile)\n",
    "        \n",
    "     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
